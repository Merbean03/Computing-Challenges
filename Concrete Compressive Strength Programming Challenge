{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "780f98fd",
   "metadata": {},
   "source": [
    "# <u>**Coding Challenge 2023 / 2024: Concrete Rules**</u>\n",
    "#### Group (6) Members: Aakash Dharmaraj, Ry Nduma, Felix Watson, Mervyn Ochoa-Dugoy\n",
    "# **Overview**\n",
    "\n",
    "**Preprocessing:**\n",
    "\n",
    "A class called **PreProcessing** was implemented, which includes functions such as <i>FillNaN</i> and <i>Transform</i>. For <i>FillNaN</i>, the classification method **KNNInputer** from the sklearn.impute module was used to replace all missing (NaN) values for each variable by selecting the best appropriate number according to the nearest neighbours of the given data point. The effect on the data was visualised through distribution plots and scatter graphs, observing trends. These observations were then used to apply appropriate <u>transformations</u> (e.g. Logarithmic) such that each variable displayed the best possible linear relationship with the the output variable, <u>compressive strength</u>. \n",
    "\n",
    "To ensure the best linear relationship, a <u>skewness score</u> was implemented to check how well distributed the variable data was, more information about which can be found here: https://pyshark.com/skewness-in-python/. This is useful as normality plays a crucial role in the performance of regression models later on.\n",
    "\n",
    "**Regression Model**\n",
    "\n",
    "Prior to performing the regression models, the data was split into training and test sets using the <i>train_test_split</i> function from the sklearn.model.selection module. Using a class called **RegressionModel**, cross validation scores were compared using heatmaps to find suitable parameters, or <u>hyperparameters</u>, for each regression method. These optimised parameters were visualised in plots and error analysis and thus can be used to find a rational regression method, as well as other practical considerations. By using this regression method, the <u>coefficients</u> for each input variable can be obtained for the interactive graph.\n",
    "\n",
    "**Interactive Graph**\n",
    "\n",
    "Using the coefficients obtained by the chosen regression method, and using the transformations applied during pre-processing, an interactive graph was plotted, which describes the relationships each <u>input variable</u> has with the <u>compressive strength</u> of concrete. Using sliders, the effect of the mixture on the final compressive strength can be visualised.\n",
    "\n",
    "(*Note*: specific comments are included in cells and markdowns for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a84ef06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:10.584294600Z",
     "start_time": "2024-01-10T23:23:09.451600100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell so the rest of the code can work!\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # ignores any warnings that appear from libraries, e.g. seaborn.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import explained_variance_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from matplotlib.widgets import Button, Slider"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23250798",
   "metadata": {},
   "source": [
    "# ** Constructing the Preprocessing Class**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1e4b5b6c667eb73",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3abcbb77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:10.627327100Z",
     "start_time": "2024-01-10T23:23:10.598265Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = 'Concrete_Data_Yeh_final.csv'\n",
    "# Input variables: cement, slag, flyash, water, superplasticizer, coarseaggregate, fineaggregate, age\n",
    "# Output variable: csMPa\n",
    "\n",
    "class PreProcessing:\n",
    "    '''Aim of Class: To successfully modify the file given, with the most appropriate preprocessing methods. \n",
    "    This includes either replacing or removing the NaN values, and applying relevant transformations to the data.'''\n",
    "    def __init__(self, file):\n",
    "        # Initialises the class with the file name.\n",
    "        self.data = pd.read_csv(file)\n",
    "\n",
    "    def checkNaN (self):\n",
    "        # To check the number of NaN values from each variable.\n",
    "        return self.data.isnull().sum()\n",
    "\n",
    "    def FillNaN(self, method='knn') -> pd.DataFrame:\n",
    "        # Fills each variables missing values with specified method.\n",
    "        if method == 'mean':\n",
    "            # Replaces all column NaN values with their respective mean (considered, but not used).\n",
    "            for variable in self.data.columns[:-1]:\n",
    "                self.data[variable].fillna(self.data[variable].mean(), inplace = True)\n",
    "\n",
    "        elif method == 'median':\n",
    "            # Replaces all column NaN values with their respective median (considered, but not used).\n",
    "            for variable in self.data.columns[:-1]:\n",
    "                self.data[variable].fillna(self.data[variable].median(), inplace = True)\n",
    "\n",
    "        elif method == 'knn':\n",
    "            # Replaces all column NaN values with values predicted by KNN imputer classification method (Considered and used).\n",
    "            data = self.data.values\n",
    "\n",
    "            # Split into input and output elements and print total missing prior to KNN. \n",
    "            ix = [i for i in range(data.shape[1]) if i != 8]\n",
    "            X, y = data[:, ix], data[:, 8]\n",
    "            print('Total Data Values Missing: %d' % sum(np.isnan(X).flatten()))\n",
    "\n",
    "            # Initiates KNNImputer to replace NaN values with predicted values and checking total missing after to see if it was successful.\n",
    "            imputer = KNNImputer()\n",
    "            imputer.fit(X)\n",
    "            Xtrans = imputer.transform(X)\n",
    "            print('Total Data Values Missing after imputing KNN: %d' % sum(np.isnan(Xtrans).flatten()))\n",
    "\n",
    "            # Replaces the original data with the imputed data.\n",
    "            df= pd.DataFrame(Xtrans)\n",
    "            df.insert(len(df.columns), 'csMPa', y)\n",
    "            df.columns = self.data.columns\n",
    "            self.data = df\n",
    "\n",
    "        else:\n",
    "            # Raises an error if the method is not recognised.\n",
    "            raise ValueError(\"Invalid imputation method.\")\n",
    "\n",
    "        return self.data\n",
    "\n",
    "    def RemoveNaN (self) -> pd.DataFrame:\n",
    "        # Removes any row that contains any NaN values from the original dataset (considered, but not used).\n",
    "        return self.data.dropna(inplace = True)\n",
    "\n",
    "    def Transform (self) -> pd.DataFrame:\n",
    "        # Applies relevant transformations to the data for specified columns for optimisation in regression, given FillNaN is used.\n",
    "        trandata = self.FillNaN()\n",
    "        trandata['cement'] = trandata['cement'].apply(lambda x: np.sqrt(x))\n",
    "        trandata['slag'] = trandata['slag'].apply(lambda x:np.log(x+1))\n",
    "        trandata['superplasticizer'] = trandata['superplasticizer'].apply(lambda x: np.sqrt(x**(4/3)))\n",
    "        trandata['age'] = trandata['age'].apply(lambda x: np.log(x))\n",
    "        return trandata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "883b0d95df6e63af",
   "metadata": {},
   "source": [
    "### Count the number of NaN values in each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e95d11e17934d917",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:10.673864800Z",
     "start_time": "2024-01-10T23:23:10.607729400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cement               0\n",
       "slag                 6\n",
       "flyash               1\n",
       "water                8\n",
       "superplasticizer    14\n",
       "coarseaggregate      7\n",
       "fineaggregate        3\n",
       "age                  5\n",
       "csMPa                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for existing NaN values in the dataset.\n",
    "checkNaNs = PreProcessing(filename).checkNaN()\n",
    "checkNaNs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c569b8480def356",
   "metadata": {},
   "source": [
    "### Processed Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d36361eaac5ca73e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:10.844914Z",
     "start_time": "2024-01-10T23:23:10.646490100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Values Missing: 44\n",
      "Total Data Values Missing after imputing KNN: 0\n",
      "Total Data Values Missing: 44\n",
      "Total Data Values Missing after imputing KNN: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.00000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.847903</td>\n",
       "      <td>54.19866</td>\n",
       "      <td>181.570369</td>\n",
       "      <td>6.215184</td>\n",
       "      <td>972.784699</td>\n",
       "      <td>773.580252</td>\n",
       "      <td>45.428155</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.220315</td>\n",
       "      <td>64.00873</td>\n",
       "      <td>21.271200</td>\n",
       "      <td>5.964938</td>\n",
       "      <td>77.717326</td>\n",
       "      <td>80.159764</td>\n",
       "      <td>62.371898</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.30000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.10000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cement         slag      flyash        water  superplasticizer  \\\n",
       "count  1030.000000  1030.000000  1030.00000  1030.000000       1030.000000   \n",
       "mean    281.167864    73.847903    54.19866   181.570369          6.215184   \n",
       "std     104.506364    86.220315    64.00873    21.271200          5.964938   \n",
       "min     102.000000     0.000000     0.00000   121.800000          0.000000   \n",
       "25%     192.375000     0.000000     0.00000   164.900000          0.000000   \n",
       "50%     272.900000    22.000000     0.00000   185.000000          6.400000   \n",
       "75%     350.000000   142.950000   118.30000   192.000000         10.200000   \n",
       "max     540.000000   359.400000   200.10000   247.000000         32.200000   \n",
       "\n",
       "       coarseaggregate  fineaggregate          age        csMPa  \n",
       "count      1030.000000    1030.000000  1030.000000  1030.000000  \n",
       "mean        972.784699     773.580252    45.428155    35.817961  \n",
       "std          77.717326      80.159764    62.371898    16.705742  \n",
       "min         801.000000     594.000000     1.000000     2.330000  \n",
       "25%         932.000000     730.950000     7.000000    23.710000  \n",
       "50%         968.000000     779.500000    28.000000    34.445000  \n",
       "75%        1029.400000     824.000000    56.000000    46.135000  \n",
       "max        1145.000000     992.600000   365.000000    82.600000  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''For reference, preprocessed: CD = Concrete Data and TCD = Transformed Concrete Data and DIG = Data for Interactive Graph'''\n",
    "CD = PreProcessing(filename).FillNaN('knn') # Fills all NaN values with KNN imputer.\n",
    "DIG = PreProcessing(filename).FillNaN('knn')\n",
    "CD.describe()   # General statistics of the data for each variable in the original database."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbdefe7c9850392f",
   "metadata": {},
   "source": [
    "### Transformed Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53a01b34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:11.051141200Z",
     "start_time": "2024-01-10T23:23:10.795047400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Data Values Missing: 44\n",
      "Total Data Values Missing after imputing KNN: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>flyash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplasticizer</th>\n",
       "      <th>coarseaggregate</th>\n",
       "      <th>fineaggregate</th>\n",
       "      <th>age</th>\n",
       "      <th>csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.00000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16.479268</td>\n",
       "      <td>2.553403</td>\n",
       "      <td>54.19866</td>\n",
       "      <td>181.570369</td>\n",
       "      <td>2.841409</td>\n",
       "      <td>972.784699</td>\n",
       "      <td>773.580252</td>\n",
       "      <td>3.168714</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.100148</td>\n",
       "      <td>2.411536</td>\n",
       "      <td>64.00873</td>\n",
       "      <td>21.271200</td>\n",
       "      <td>2.413060</td>\n",
       "      <td>77.717326</td>\n",
       "      <td>80.159764</td>\n",
       "      <td>1.184658</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10.099505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>13.869912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>1.945910</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16.519685</td>\n",
       "      <td>3.135494</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>3.447096</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>3.332205</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18.708287</td>\n",
       "      <td>4.969466</td>\n",
       "      <td>118.30000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>4.703272</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>4.025352</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>23.237900</td>\n",
       "      <td>5.887215</td>\n",
       "      <td>200.10000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>10.121322</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>5.899897</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cement         slag      flyash        water  superplasticizer  \\\n",
       "count  1030.000000  1030.000000  1030.00000  1030.000000       1030.000000   \n",
       "mean     16.479268     2.553403    54.19866   181.570369          2.841409   \n",
       "std       3.100148     2.411536    64.00873    21.271200          2.413060   \n",
       "min      10.099505     0.000000     0.00000   121.800000          0.000000   \n",
       "25%      13.869912     0.000000     0.00000   164.900000          0.000000   \n",
       "50%      16.519685     3.135494     0.00000   185.000000          3.447096   \n",
       "75%      18.708287     4.969466   118.30000   192.000000          4.703272   \n",
       "max      23.237900     5.887215   200.10000   247.000000         10.121322   \n",
       "\n",
       "       coarseaggregate  fineaggregate          age        csMPa  \n",
       "count      1030.000000    1030.000000  1030.000000  1030.000000  \n",
       "mean        972.784699     773.580252     3.168714    35.817961  \n",
       "std          77.717326      80.159764     1.184658    16.705742  \n",
       "min         801.000000     594.000000     0.000000     2.330000  \n",
       "25%         932.000000     730.950000     1.945910    23.710000  \n",
       "50%         968.000000     779.500000     3.332205    34.445000  \n",
       "75%        1029.400000     824.000000     4.025352    46.135000  \n",
       "max        1145.000000     992.600000     5.899897    82.600000  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TCD = PreProcessing(filename).Transform()   # Applies transformations to the data.\n",
    "TCD.describe() # General statistics of the data for each variable in the transformed ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cb7ceef",
   "metadata": {},
   "source": [
    "### Correlation Matrix\n",
    "\n",
    "Typically a correlation matrix is used to display the strength and direction of the linear relationship between each variable. Looking at the \"csMPa\" or <i>compressive strength</i> row, it is observed that the variables with the greatest correlations with strength are <u>cement</u>, <u>superplasticizer</u> and <u>age</u>, all having positive correlations also. Conversely, <u>water</u> has the most negative correlation with strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "656190f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:11.407290400Z",
     "start_time": "2024-01-10T23:23:10.889213900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7389a_row0_col0, #T_7389a_row1_col1, #T_7389a_row2_col2, #T_7389a_row3_col3, #T_7389a_row4_col4, #T_7389a_row5_col5, #T_7389a_row6_col6, #T_7389a_row7_col7, #T_7389a_row8_col8 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row0_col1 {\n",
       "  background-color: #455cce;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row0_col2, #T_7389a_row1_col5, #T_7389a_row2_col0, #T_7389a_row2_col1, #T_7389a_row3_col4, #T_7389a_row3_col6, #T_7389a_row3_col8, #T_7389a_row4_col3, #T_7389a_row4_col7 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row0_col3, #T_7389a_row8_col1 {\n",
       "  background-color: #aec9fc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row0_col4 {\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row0_col5 {\n",
       "  background-color: #6687ed;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row0_col6 {\n",
       "  background-color: #6c8ff1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row0_col7, #T_7389a_row3_col0 {\n",
       "  background-color: #85a8fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row0_col8 {\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row1_col0 {\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row1_col2 {\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row1_col3, #T_7389a_row4_col6 {\n",
       "  background-color: #d3dbe7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row1_col4 {\n",
       "  background-color: #c6d6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row1_col6 {\n",
       "  background-color: #5e7de7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row1_col7, #T_7389a_row6_col3 {\n",
       "  background-color: #6180e9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row1_col8 {\n",
       "  background-color: #a9c6fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row2_col3 {\n",
       "  background-color: #89acfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row2_col4 {\n",
       "  background-color: #f5c4ac;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row2_col5 {\n",
       "  background-color: #81a4fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row2_col6 {\n",
       "  background-color: #b5cdfa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row2_col7, #T_7389a_row6_col1, #T_7389a_row6_col7 {\n",
       "  background-color: #445acc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row2_col8 {\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row3_col1 {\n",
       "  background-color: #a7c5fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row3_col2 {\n",
       "  background-color: #5977e3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row3_col5 {\n",
       "  background-color: #536edd;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row3_col7 {\n",
       "  background-color: #bed2f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row4_col0 {\n",
       "  background-color: #afcafc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row4_col1 {\n",
       "  background-color: #96b7ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row4_col2 {\n",
       "  background-color: #ead4c8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row4_col5 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row4_col8 {\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row5_col0, #T_7389a_row7_col6 {\n",
       "  background-color: #7da0f9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row5_col1 {\n",
       "  background-color: #4257c9;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row5_col2, #T_7389a_row7_col4 {\n",
       "  background-color: #97b8ff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row5_col3 {\n",
       "  background-color: #9abbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row5_col4 {\n",
       "  background-color: #88abfd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row5_col6 {\n",
       "  background-color: #7699f6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row5_col7 {\n",
       "  background-color: #6e90f2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row5_col8, #T_7389a_row6_col8, #T_7389a_row8_col5 {\n",
       "  background-color: #5875e1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row6_col0 {\n",
       "  background-color: #6282ea;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row6_col2, #T_7389a_row7_col0 {\n",
       "  background-color: #adc9fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row6_col4 {\n",
       "  background-color: #e4d9d2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row6_col5 {\n",
       "  background-color: #5470de;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row7_col1, #T_7389a_row8_col2 {\n",
       "  background-color: #7ea1fa;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row7_col2 {\n",
       "  background-color: #7295f4;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row7_col3 {\n",
       "  background-color: #ebd3c6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row7_col5 {\n",
       "  background-color: #84a7fc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row7_col8 {\n",
       "  background-color: #d8dce2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row8_col0 {\n",
       "  background-color: #f6bfa6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row8_col3 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row8_col4 {\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_7389a_row8_col6 {\n",
       "  background-color: #799cf8;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_7389a_row8_col7 {\n",
       "  background-color: #ccd9ed;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7389a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7389a_level0_col0\" class=\"col_heading level0 col0\" >cement</th>\n",
       "      <th id=\"T_7389a_level0_col1\" class=\"col_heading level0 col1\" >slag</th>\n",
       "      <th id=\"T_7389a_level0_col2\" class=\"col_heading level0 col2\" >flyash</th>\n",
       "      <th id=\"T_7389a_level0_col3\" class=\"col_heading level0 col3\" >water</th>\n",
       "      <th id=\"T_7389a_level0_col4\" class=\"col_heading level0 col4\" >superplasticizer</th>\n",
       "      <th id=\"T_7389a_level0_col5\" class=\"col_heading level0 col5\" >coarseaggregate</th>\n",
       "      <th id=\"T_7389a_level0_col6\" class=\"col_heading level0 col6\" >fineaggregate</th>\n",
       "      <th id=\"T_7389a_level0_col7\" class=\"col_heading level0 col7\" >age</th>\n",
       "      <th id=\"T_7389a_level0_col8\" class=\"col_heading level0 col8\" >csMPa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row0\" class=\"row_heading level0 row0\" >cement</th>\n",
       "      <td id=\"T_7389a_row0_col0\" class=\"data row0 col0\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row0_col1\" class=\"data row0 col1\" >-0.275391</td>\n",
       "      <td id=\"T_7389a_row0_col2\" class=\"data row0 col2\" >-0.397501</td>\n",
       "      <td id=\"T_7389a_row0_col3\" class=\"data row0 col3\" >-0.080717</td>\n",
       "      <td id=\"T_7389a_row0_col4\" class=\"data row0 col4\" >0.090211</td>\n",
       "      <td id=\"T_7389a_row0_col5\" class=\"data row0 col5\" >-0.108566</td>\n",
       "      <td id=\"T_7389a_row0_col6\" class=\"data row0 col6\" >-0.222729</td>\n",
       "      <td id=\"T_7389a_row0_col7\" class=\"data row0 col7\" >0.079688</td>\n",
       "      <td id=\"T_7389a_row0_col8\" class=\"data row0 col8\" >0.497832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row1\" class=\"row_heading level0 row1\" >slag</th>\n",
       "      <td id=\"T_7389a_row1_col0\" class=\"data row1 col0\" >-0.275391</td>\n",
       "      <td id=\"T_7389a_row1_col1\" class=\"data row1 col1\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row1_col2\" class=\"data row1 col2\" >-0.323409</td>\n",
       "      <td id=\"T_7389a_row1_col3\" class=\"data row1 col3\" >0.109913</td>\n",
       "      <td id=\"T_7389a_row1_col4\" class=\"data row1 col4\" >0.040992</td>\n",
       "      <td id=\"T_7389a_row1_col5\" class=\"data row1 col5\" >-0.287225</td>\n",
       "      <td id=\"T_7389a_row1_col6\" class=\"data row1 col6\" >-0.281997</td>\n",
       "      <td id=\"T_7389a_row1_col7\" class=\"data row1 col7\" >-0.046032</td>\n",
       "      <td id=\"T_7389a_row1_col8\" class=\"data row1 col8\" >0.133628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row2\" class=\"row_heading level0 row2\" >flyash</th>\n",
       "      <td id=\"T_7389a_row2_col0\" class=\"data row2 col0\" >-0.397501</td>\n",
       "      <td id=\"T_7389a_row2_col1\" class=\"data row2 col1\" >-0.323409</td>\n",
       "      <td id=\"T_7389a_row2_col2\" class=\"data row2 col2\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row2_col3\" class=\"data row2 col3\" >-0.258555</td>\n",
       "      <td id=\"T_7389a_row2_col4\" class=\"data row2 col4\" >0.381499</td>\n",
       "      <td id=\"T_7389a_row2_col5\" class=\"data row2 col5\" >-0.008929</td>\n",
       "      <td id=\"T_7389a_row2_col6\" class=\"data row2 col6\" >0.079306</td>\n",
       "      <td id=\"T_7389a_row2_col7\" class=\"data row2 col7\" >-0.153183</td>\n",
       "      <td id=\"T_7389a_row2_col8\" class=\"data row2 col8\" >-0.105841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row3\" class=\"row_heading level0 row3\" >water</th>\n",
       "      <td id=\"T_7389a_row3_col0\" class=\"data row3 col0\" >-0.080717</td>\n",
       "      <td id=\"T_7389a_row3_col1\" class=\"data row3 col1\" >0.109913</td>\n",
       "      <td id=\"T_7389a_row3_col2\" class=\"data row3 col2\" >-0.258555</td>\n",
       "      <td id=\"T_7389a_row3_col3\" class=\"data row3 col3\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row3_col4\" class=\"data row3 col4\" >-0.656377</td>\n",
       "      <td id=\"T_7389a_row3_col5\" class=\"data row3 col5\" >-0.181684</td>\n",
       "      <td id=\"T_7389a_row3_col6\" class=\"data row3 col6\" >-0.450139</td>\n",
       "      <td id=\"T_7389a_row3_col7\" class=\"data row3 col7\" >0.274422</td>\n",
       "      <td id=\"T_7389a_row3_col8\" class=\"data row3 col8\" >-0.291351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row4\" class=\"row_heading level0 row4\" >superplasticizer</th>\n",
       "      <td id=\"T_7389a_row4_col0\" class=\"data row4 col0\" >0.090211</td>\n",
       "      <td id=\"T_7389a_row4_col1\" class=\"data row4 col1\" >0.040992</td>\n",
       "      <td id=\"T_7389a_row4_col2\" class=\"data row4 col2\" >0.381499</td>\n",
       "      <td id=\"T_7389a_row4_col3\" class=\"data row4 col3\" >-0.656377</td>\n",
       "      <td id=\"T_7389a_row4_col4\" class=\"data row4 col4\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row4_col5\" class=\"data row4 col5\" >-0.263930</td>\n",
       "      <td id=\"T_7389a_row4_col6\" class=\"data row4 col6\" >0.222986</td>\n",
       "      <td id=\"T_7389a_row4_col7\" class=\"data row4 col7\" >-0.192717</td>\n",
       "      <td id=\"T_7389a_row4_col8\" class=\"data row4 col8\" >0.365028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row5\" class=\"row_heading level0 row5\" >coarseaggregate</th>\n",
       "      <td id=\"T_7389a_row5_col0\" class=\"data row5 col0\" >-0.108566</td>\n",
       "      <td id=\"T_7389a_row5_col1\" class=\"data row5 col1\" >-0.287225</td>\n",
       "      <td id=\"T_7389a_row5_col2\" class=\"data row5 col2\" >-0.008929</td>\n",
       "      <td id=\"T_7389a_row5_col3\" class=\"data row5 col3\" >-0.181684</td>\n",
       "      <td id=\"T_7389a_row5_col4\" class=\"data row5 col4\" >-0.263930</td>\n",
       "      <td id=\"T_7389a_row5_col5\" class=\"data row5 col5\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row5_col6\" class=\"data row5 col6\" >-0.178383</td>\n",
       "      <td id=\"T_7389a_row5_col7\" class=\"data row5 col7\" >0.000862</td>\n",
       "      <td id=\"T_7389a_row5_col8\" class=\"data row5 col8\" >-0.165439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row6\" class=\"row_heading level0 row6\" >fineaggregate</th>\n",
       "      <td id=\"T_7389a_row6_col0\" class=\"data row6 col0\" >-0.222729</td>\n",
       "      <td id=\"T_7389a_row6_col1\" class=\"data row6 col1\" >-0.281997</td>\n",
       "      <td id=\"T_7389a_row6_col2\" class=\"data row6 col2\" >0.079306</td>\n",
       "      <td id=\"T_7389a_row6_col3\" class=\"data row6 col3\" >-0.450139</td>\n",
       "      <td id=\"T_7389a_row6_col4\" class=\"data row6 col4\" >0.222986</td>\n",
       "      <td id=\"T_7389a_row6_col5\" class=\"data row6 col5\" >-0.178383</td>\n",
       "      <td id=\"T_7389a_row6_col6\" class=\"data row6 col6\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row6_col7\" class=\"data row6 col7\" >-0.151734</td>\n",
       "      <td id=\"T_7389a_row6_col8\" class=\"data row6 col8\" >-0.167119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row7\" class=\"row_heading level0 row7\" >age</th>\n",
       "      <td id=\"T_7389a_row7_col0\" class=\"data row7 col0\" >0.079688</td>\n",
       "      <td id=\"T_7389a_row7_col1\" class=\"data row7 col1\" >-0.046032</td>\n",
       "      <td id=\"T_7389a_row7_col2\" class=\"data row7 col2\" >-0.153183</td>\n",
       "      <td id=\"T_7389a_row7_col3\" class=\"data row7 col3\" >0.274422</td>\n",
       "      <td id=\"T_7389a_row7_col4\" class=\"data row7 col4\" >-0.192717</td>\n",
       "      <td id=\"T_7389a_row7_col5\" class=\"data row7 col5\" >0.000862</td>\n",
       "      <td id=\"T_7389a_row7_col6\" class=\"data row7 col6\" >-0.151734</td>\n",
       "      <td id=\"T_7389a_row7_col7\" class=\"data row7 col7\" >1.000000</td>\n",
       "      <td id=\"T_7389a_row7_col8\" class=\"data row7 col8\" >0.329289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7389a_level0_row8\" class=\"row_heading level0 row8\" >csMPa</th>\n",
       "      <td id=\"T_7389a_row8_col0\" class=\"data row8 col0\" >0.497832</td>\n",
       "      <td id=\"T_7389a_row8_col1\" class=\"data row8 col1\" >0.133628</td>\n",
       "      <td id=\"T_7389a_row8_col2\" class=\"data row8 col2\" >-0.105841</td>\n",
       "      <td id=\"T_7389a_row8_col3\" class=\"data row8 col3\" >-0.291351</td>\n",
       "      <td id=\"T_7389a_row8_col4\" class=\"data row8 col4\" >0.365028</td>\n",
       "      <td id=\"T_7389a_row8_col5\" class=\"data row8 col5\" >-0.165439</td>\n",
       "      <td id=\"T_7389a_row8_col6\" class=\"data row8 col6\" >-0.167119</td>\n",
       "      <td id=\"T_7389a_row8_col7\" class=\"data row8 col7\" >0.329289</td>\n",
       "      <td id=\"T_7389a_row8_col8\" class=\"data row8 col8\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x280b7bb50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CD.corr().style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fabd3f16",
   "metadata": {},
   "source": [
    "### Scaling the Data\n",
    "Here variables were scaled, ensuring suitability for later regression techniques. Distribution and scatter plots were also generated for each scaled variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f065f943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:16.933234900Z",
     "start_time": "2024-01-10T23:23:11.088980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNon-transformed skewness data\n",
      "\u001b[0m            Variable  Skewness\n",
      "0            cement  0.508739\n",
      "1              slag  0.800379\n",
      "2            flyash  0.536419\n",
      "3             water  0.077448\n",
      "4  superplasticizer  0.897946\n",
      "5   coarseaggregate -0.036001\n",
      "6     fineaggregate -0.253097\n",
      "7               age  3.273802\n"
     ]
    }
   ],
   "source": [
    "# Scales all input variables using MinMaxScaler from sklearn.preprocessing (applied to both CD and TCD).\n",
    "scaler = MinMaxScaler()\n",
    "CD.iloc[:,:-1] = scaler.fit_transform(CD.iloc[:,:-1])\n",
    "\n",
    "# Plotting distribution and scatter plots for the non-transformed data to observe initial trends and skewness.\n",
    "fig, axs = plt.subplots(2, 8, figsize=(40, 10))\n",
    "fig.suptitle('Non-transformed Distribution and Scatter plots for each input variable (scaled)', y = 1, fontsize = 40)\n",
    "\n",
    "df = {'Variable': [], 'Skewness': []}\n",
    "for i, input in enumerate(CD.columns[:-1]):\n",
    "    # Adds the name of the variable and its skewness to the df dictionary.\n",
    "    df['Variable'].append(input)\n",
    "    df['Skewness'].append(skew(CD[input]))\n",
    "\n",
    "    # Plots the distribution and scatter plots for each variable.\n",
    "    sns.distplot(CD[input], ax=axs[0, i])\n",
    "    axs[0,i].axvline(CD[input].mean(),linestyle=\"dashed\",label=\"mean\", color='black')\n",
    "    sns.scatterplot(x=CD[input], y=CD['csMPa'], ax=axs[1, i], color = 'red', s = 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prints the skewness of each variable of CD in a dataframe.\n",
    "print('\\033[1m' + 'Non-transformed skewness data\\n' + '\\033[0m', pd.DataFrame(df))  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2bd92323",
   "metadata": {},
   "source": [
    "### Interpreting the pre-processed data\n",
    "Looking at all the evidence (correlation matrix, distribution plots, scatter graphs, skewness results and the scaled DataFrame), it was decided best to proceed with the following variables (brief reasons are included):\n",
    "- **Cement**: <u>Square Root</u> transformation - the distribution plot became slightly more gaussian; skewness due to high density of low values was reduced.\n",
    "- **Slag**: <u>Logarithmic</u> transformation - emphasises the frequency of non zero values; the distribution became more normalised.\n",
    "- **Flyash**: <u>REMOVED</u> - caused multicolinearity with other input variables and had the lowest gradient relationship with compressive strength. The presence of a lot of noise in the scatter plot also supports this conclusion.\n",
    "- **Water**: <u>No</u> transformation - The skewness result was close to zero, signifying near-perfect normality of data.\n",
    "- **Superplasticizer**: <u>Square Root</u> transformation - a strong positive trend occurred at low superplasticizer values; became less clear if continued at higher values.\n",
    "- **Coarse aggregate**: <u>No</u> transformation - no defined trend was seen, there was a wide spread of data points.\n",
    "- **Fine aggregate**: <u>No</u> transformation - slight negative correlation with compressive strength, displayed initial linearity regardless.\n",
    "- **Age**: <u>Logarithmic</u> transformation - a curved increase in strength with a slight plateau observed in scatter plot.\n",
    "\n",
    "**These transformations are seen in the following code below.**\n",
    "\n",
    "##### Other notable observations:\n",
    "1. Applying **appropriate transformations** (square root, logarithmic and reciprocal) to columns leads to a more *defined* distibution plot with the data initially held in the database. It is generally observed that **the closer these plots are to the normal gaussian distribution, the closer the data is to the ideal linear relationship** between the input variable and the compressive strength.\n",
    "\n",
    "2. A few distribution plots, particularly **slag**, **flyash** and **superplasticizer**, have *2* gaussians. This is due to the high amount of null (NaN) values provided from the document. Considering these would skew the regression fit and not represent a reasonable trend with the compressive strength.\n",
    "\n",
    "3. **Skewness values** were used because, for linear relationships, a **normalised distribution** of data is assumed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "199482c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:21.848685100Z",
     "start_time": "2024-01-10T23:23:16.933234900Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTransformed skewness data\n",
      "\u001b[0m            Variable  Skewness\n",
      "0            cement  0.189916\n",
      "1              slag -0.032737\n",
      "2             water  0.077448\n",
      "3  superplasticizer  0.117379\n",
      "4   coarseaggregate -0.036001\n",
      "5     fineaggregate -0.253097\n",
      "6               age -0.161967\n"
     ]
    }
   ],
   "source": [
    "# Scales all input variables using MinMaxScaler, and removes 'flyash' data from TCD from our observations.\n",
    "TCD.iloc[:,:-1] = scaler.fit_transform(TCD.iloc[:,:-1])\n",
    "if 'flyash' in TCD.columns:\n",
    "    TCD.drop('flyash', axis=1, inplace=True)\n",
    "\n",
    "# Plotting distribution and scatter plots for the transformed data to observe initial trends and skewness.\n",
    "fig, axs = plt.subplots(2, (len(TCD.columns)-1), figsize=(40, 12))\n",
    "fig.suptitle('Transformed Distribution and Scatter plots for each input variable (scaled)', y = 1, fontsize = 40)\n",
    "\n",
    "df = {'Variable': [], 'Skewness': []}\n",
    "for i, input in enumerate(TCD.columns[:-1]):\n",
    "    # Adds the name of the variable and its skewness to the df dictionary.\n",
    "    df['Variable'].append(input)\n",
    "    df['Skewness'].append(skew(TCD[input]))\n",
    "\n",
    "    # Plots the distribution and scatter plots for each variable.\n",
    "    sns.distplot(TCD[input], ax=axs[0, i])\n",
    "    axs[0,i].axvline(TCD[input].mean(),linestyle=\"dashed\",label=\"mean\", color='black')\n",
    "    sns.scatterplot(x=TCD[input], y=TCD['csMPa'], ax=axs[1, i], color = 'red', s = 5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prints the skewness of each variable of TCD in a dataframe.\n",
    "print('\\033[1m' + 'Transformed skewness data\\n' + '\\033[0m', pd.DataFrame(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee4e457e",
   "metadata": {},
   "source": [
    "# **Regression Model**\n",
    "##### The following regression models were considered : \n",
    "- Linear\n",
    "- Ridge\n",
    "- Lasso\n",
    "- Random Forest \n",
    "- Support Vector Regression (SVR).\n",
    "\n",
    "The training data encompassed **80%** of the database, with the remaining **20%** used for testing. This provided <u>optimal regression</u> and <u>high scores</u> (NB: this did not deduce which regressions were excluded)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4a428639df103ac",
   "metadata": {},
   "source": [
    "### Splitting database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06bda333",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:21.849682100Z",
     "start_time": "2024-01-10T23:23:21.848685100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y:\n",
      "[79.99 61.89 40.27 ... 23.7  32.77 32.4 ]\n",
      "\n",
      "X:\n",
      "             0         1         2         3         4         5         6\n",
      "0     1.000000  0.000000  0.321086  0.181994  0.694767  0.205720  0.564790\n",
      "1     1.000000  0.000000  0.321086  0.181994  0.738372  0.205720  0.564790\n",
      "2     0.619184  0.843580  0.848243  0.000000  0.380814  0.000000  0.948902\n",
      "3     0.619184  0.843580  0.848243  0.000000  0.380814  0.000000  0.804240\n",
      "4     0.303922  0.831183  0.560703  0.000000  0.515698  0.580783  0.997662\n",
      "...        ...       ...       ...       ...       ...       ...       ...\n",
      "1025  0.496695  0.808901  0.461661  0.424316  0.200872  0.437280  0.564790\n",
      "1026  0.597518  0.000000  0.592652  0.470744  0.049128  0.550426  0.564790\n",
      "1027  0.158813  0.839870  0.566294  0.329850  0.265698  0.466633  0.564790\n",
      "1028  0.191346  0.889189  0.429712  0.497525  0.548256  0.488961  0.564790\n",
      "1029  0.460703  0.784761  0.629393  0.414726  0.184593  0.420221  0.564790\n",
      "\n",
      "[1030 rows x 7 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting the DataFrame into training and testing sets using sklearn's train_test_split function.\n",
    "X = TCD.iloc[:,:-1].to_numpy() \n",
    "y = TCD['csMPa'].to_numpy()\n",
    "\n",
    "print(f'y:\\n{y}\\n')\n",
    "print(f'X:\\n{pd.DataFrame(X)}\\n') # Checking the data is split correctly (X = 2D array, y = 1D array).\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e17fcdba510bbb5",
   "metadata": {},
   "source": [
    "### Checking the general shape of each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22b7bd83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:21.911013900Z",
     "start_time": "2024-01-10T23:23:21.855934400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(206,) (824,) (206, 7) (824, 7)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shape of both the training and testing sets are reasonable.\n",
    "print(y_test.shape, y_train.shape, x_test.shape, x_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1946bc723cb3bbe2",
   "metadata": {},
   "source": [
    "### Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d811fa73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:23.113285700Z",
     "start_time": "2024-01-10T23:23:21.862916Z"
    }
   },
   "outputs": [],
   "source": [
    "class RegressionModel:\n",
    "    '''The aim of this class is to successfully fit the regression model to the training data, predict the y values for the \n",
    "    testing data, and evaluate the model using the specified metrics and scores. The regressions can be visualised with plots.'''\n",
    "    def __init__(self, x_train, x_test, y_train, y_test, regression, **kwargs):\n",
    "        self.x_train = x_train\n",
    "        self.x_test = x_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "        self.regression = regression(**kwargs)                  # Initiates regression method with any required arguments.\n",
    "        self.regfit = self.regression.fit(x_train, y_train)     # Fits the regression method to the training data.\n",
    "        self.y_pred = self.regression.predict(x_test)           # Predicts the y values for the testing data.\n",
    "    \n",
    "    def Metric(self, metriclist) -> pd.DataFrame:\n",
    "        # Performs error analysis on the given regression model using the specified metrics.\n",
    "        metricdf = {'Metric': [], 'Score': []}\n",
    "        for metric in metriclist:\n",
    "            metricdf['Metric'].append(metric.__name__)\n",
    "            if metric == cross_val_score:\n",
    "                # Takes the mean of the cross validation scores using the training data.\n",
    "                metricdf['Score'].append(metric(self.regression, self.x_train, self.y_train, cv=5).mean())\n",
    "            else:\n",
    "                # Takes the metric score for the testing data.\n",
    "                metricdf['Score'].append(metric(self.y_test, self.y_pred))\n",
    "        return pd.DataFrame(metricdf)\n",
    "    \n",
    "    def RegScores(self) -> pd.Series:\n",
    "        # Compares the regression model scores between the training and testing data.\n",
    "        difference = np.abs(self.regression.score(self.x_test, self.y_test) - self.regression.score(self.x_train, self.y_train))\n",
    "        scores = {'Training Score': self.regression.score(self.x_train, self.y_train), 'Testing Score': self.regression.score(self.x_test, self.y_test), 'Difference': difference.round(4)}\n",
    "        return pd.Series(scores)\n",
    "\n",
    "    def Plot(self):\n",
    "        # Visualises the regression model with the given parameters.\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(self.y_test, self.y_pred, color='red', s=5)\n",
    "        plt.title(f'{self.regression.__class__.__name__} | Score: {self.regression.score(self.x_test, self.y_test)}')\n",
    "        plt.plot(y_test,y_test, color='black', linestyle = 'dashed')\n",
    "        plt.xlabel('Actual csMPa')\n",
    "        plt.ylabel('Predicted csMPa')\n",
    "        plt.show()\n",
    "        return\n",
    "    \n",
    "    def Coefficients(self)-> np.ndarray:\n",
    "        # Returns the coefficients for each input variable of the regression model.\n",
    "        try:\n",
    "            if self.regression.__class__.__name__ in ['LinearRegression', 'Ridge', 'Lasso', 'SVR']:\n",
    "                coeffs = self.regression.coef_\n",
    "            else:\n",
    "                coeffs = self.regression.feature_importances_\n",
    "        except AttributeError:\n",
    "            coeffs = None\n",
    "        return coeffs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98408f8b",
   "metadata": {},
   "source": [
    "### Finding optimal hyperparameters\n",
    "NB: Due to their significant runtime, operate RandomForest and SVR heatmaps only if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09cce454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:23.167142500Z",
     "start_time": "2024-01-10T23:23:21.883863Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Performs the root mean squared error for error analysis when operated.'''\n",
    "def RMSE(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "alphas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "metriclist = [RMSE, explained_variance_score, mean_absolute_error, mean_squared_error, r2_score, cross_val_score] # List of metrics to be used for error analysis."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "530fa49d",
   "metadata": {},
   "source": [
    "##### <u>Linear Regression</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "94c30817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:23.276749700Z",
     "start_time": "2024-01-10T23:23:21.890070100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation scores for LinearRegression (normalised): 0.7939281928272777\n"
     ]
    }
   ],
   "source": [
    "model = RegressionModel(x_train, x_test, y_train, y_test, LinearRegression)\n",
    "cv1_score = model.Metric(metriclist).loc[model.Metric(metriclist)['Metric'] == 'cross_val_score', 'Score'].values[0]\n",
    "print(f\"Cross validation scores for LinearRegression (normalised): {cv1_score}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "11dbde8f",
   "metadata": {},
   "source": [
    "##### <u>Ridge and Lasso Regression</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9cd29bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:23:23.494644700Z",
     "start_time": "2024-01-10T23:23:21.960438300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(191.44444444444443, 0.5, 'Alpha')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CrossValscores = np.zeros((len(alphas), 2))\n",
    "for i, regression in enumerate([Ridge, Lasso]):\n",
    "    for j, alpha in enumerate(alphas):\n",
    "        if i == 0:\n",
    "            model = RegressionModel(x_train, x_test, y_train, y_test, regression, alpha = alpha)\n",
    "        else:\n",
    "            model = RegressionModel(x_train, x_test, y_train, y_test, regression, alpha = alpha, max_iter = 10000)\n",
    "        metric_df = model.Metric(metriclist)\n",
    "        cv_score = metric_df.loc[metric_df['Metric'] == 'cross_val_score', 'Score'].values[0]\n",
    "        CrossValscores[j][i] = cv_score\n",
    "\n",
    "# Plotting a heatmap of the cross validation scores for Ridge and Lasso regression.\n",
    "CrossVal_df = pd.DataFrame(CrossValscores, index=alphas, columns=['Ridge', 'Lasso'])\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(CrossVal_df, annot=True, fmt=\".6f\", cmap='Blues', ax = ax)\n",
    "ax.set_title('Cross Validation scores for Ridge and Lasso Regression')\n",
    "ax.set_xlabel('Regression')\n",
    "ax.set_ylabel('Alpha')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f499007",
   "metadata": {},
   "source": [
    "##### <u>Random Forest Regression</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f318de17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:25:49.272623300Z",
     "start_time": "2024-01-10T23:23:23.432904900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performs Random Forest Regressions for all combinations of n_estimators and max_depth, and stores the cross validation scores in a 2D array.\n",
    "RFscore_values = [[0 for _ in range(6)] for _ in range(4)] \n",
    "for i, n_estimators in enumerate([50,100,150,200]):\n",
    "    for j, max_depth in enumerate([5,10,15,20,25,30]):\n",
    "        metric_df = RegressionModel(x_train, x_test, y_train, y_test, RandomForestRegressor, n_estimators = n_estimators, max_depth = max_depth, random_state = 42).Metric(metriclist)\n",
    "        RFscore_values[i][j] = metric_df.loc[metric_df['Metric'] == 'cross_val_score', 'Score'].values[0]\n",
    "\n",
    "# Plotting a heatmap of the cross validation scores for different n_estimators and max_depth.\n",
    "RFscore_df = pd.DataFrame(RFscore_values, index=[50,100,150,200], columns=[5,10,15,20,25,30]) #\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "sns.heatmap(RFscore_df, annot=True, fmt=\".5f\", cmap='Blues', ax = ax)\n",
    "ax.set_title('Cross Validation scores for different n_estimators and max_depth')\n",
    "ax.set_xlabel('max_depth')\n",
    "ax.set_ylabel('n_estimators')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "163dad62",
   "metadata": {},
   "source": [
    "##### <u>Support Vector Regression (SVR)</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a62cbfa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:25:58.659408Z",
     "start_time": "2024-01-10T23:25:49.248732200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Performs SVR for all combinations of C and epsilon, and stores the cross validation scores in a 2D array.\n",
    "SVRscore_values = np.zeros((4,10)) #2D array\n",
    "for i, c in enumerate([0.1, 1, 10, 100]):\n",
    "    for j, epsilon in enumerate([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]):\n",
    "        metric_df = RegressionModel(x_train, x_test, y_train, y_test, SVR, kernel = 'linear', C = c, epsilon = epsilon).Metric(metriclist) \n",
    "        SVRscore_values[i][j] = metric_df.loc[metric_df['Metric'] == 'cross_val_score', 'Score'].values[0]\n",
    "\n",
    "# Plotting a heatmap of the cross validation scores for different C and epsilon.\n",
    "SVRscore_df = pd.DataFrame(SVRscore_values, index=[0.1, 1, 10, 100], columns=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "sns.heatmap(SVRscore_df, annot=True, fmt=\".5f\", cmap='Blues', ax = ax)\n",
    "ax.set_title('Cross Validation scores for different C and epsilon')\n",
    "ax.set_xlabel('epsilon')\n",
    "ax.set_ylabel('C')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6979dfc",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "The best parameters from each regression model are listed below.\n",
    "\n",
    "- <u>Linear:</u> N/A\n",
    "- <u>Ridge:</u> <i>alpha</i> = 0.5\n",
    "- <u>Random Forest:</u> <i>n_estimators</i> = 200, <i>max_depth</i> = 15\n",
    "- <u>SVR:</u> <i>C</i> = 100, <i>epsilon</i> = 0.8\n",
    "\n",
    "For Lasso Regression, the best cross validation score was observed when alpha = 0; lasso does not need to be considered anymore as it behaves identically to linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1ead72c5d9a8837b",
   "metadata": {},
   "source": [
    "### Plotting the regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d515040f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:26:12.664913Z",
     "start_time": "2024-01-10T23:25:58.661402200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score    0.798419\n",
      "Testing Score     0.818677\n",
      "Difference        0.020300\n",
      "dtype: float64\n",
      "                     Metric      Score\n",
      "0                      RMSE   6.835396\n",
      "1        mean_squared_error  46.722635\n",
      "2                  r2_score   0.818677\n",
      "3       mean_absolute_error   5.590289\n",
      "4  explained_variance_score   0.818881\n",
      "5           cross_val_score   0.793928\n",
      "Training Score    0.798206\n",
      "Testing Score     0.817980\n",
      "Difference        0.019800\n",
      "dtype: float64\n",
      "                     Metric      Score\n",
      "0                      RMSE   6.848524\n",
      "1        mean_squared_error  46.902278\n",
      "2                  r2_score   0.817980\n",
      "3       mean_absolute_error   5.599761\n",
      "4  explained_variance_score   0.818193\n",
      "5           cross_val_score   0.793890\n",
      "Training Score    0.984700\n",
      "Testing Score     0.876593\n",
      "Difference        0.108100\n",
      "dtype: float64\n",
      "                     Metric      Score\n",
      "0                      RMSE   5.639075\n",
      "1        mean_squared_error  31.799172\n",
      "2                  r2_score   0.876593\n",
      "3       mean_absolute_error   3.838944\n",
      "4  explained_variance_score   0.879320\n",
      "5           cross_val_score   0.896330\n",
      "Training Score    0.796356\n",
      "Testing Score     0.818585\n",
      "Difference        0.022200\n",
      "dtype: float64\n",
      "                     Metric      Score\n",
      "0                      RMSE   6.837134\n",
      "1        mean_squared_error  46.746404\n",
      "2                  r2_score   0.818585\n",
      "3       mean_absolute_error   5.563356\n",
      "4  explained_variance_score   0.818795\n",
      "5           cross_val_score   0.793650\n"
     ]
    }
   ],
   "source": [
    "# List of regressors and metrics to be used for the regression model.\n",
    "regressors = [LinearRegression, Ridge, RandomForestRegressor, SVR]\n",
    "metriclist = [RMSE, mean_squared_error, r2_score, mean_absolute_error, explained_variance_score, cross_val_score]\n",
    "\n",
    "for r in regressors:\n",
    "    if r is LinearRegression:\n",
    "        model = RegressionModel(x_train, x_test, y_train, y_test, r)\n",
    "    elif r is Ridge:\n",
    "        model = RegressionModel(x_train, x_test, y_train, y_test, r, alpha = 0.5, random_state = 42)\n",
    "    elif r is RandomForestRegressor:\n",
    "        model = RegressionModel(x_train, x_test, y_train, y_test, r, n_estimators = 200, max_depth = 15, random_state = 42)\n",
    "    else:\n",
    "        model = RegressionModel(x_train, x_test, y_train, y_test, r, kernel = 'linear', C = 100, epsilon = 0.8)\n",
    "    model.Plot()\n",
    "    \n",
    "    print(model.RegScores())            # To compare the regression model scores for the training and testing data.\n",
    "    print(model.Metric(metriclist))     # To evaluate the regression model error analysis using the specified metrics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c15ce61",
   "metadata": {},
   "source": [
    "### Final observations from the regression models\n",
    "With reference to the reference link below, this model provides a **'Good'** score, with predicted scores within **80-90%** across all regression models. This means the applied transformations were fairly accurate to what actual relationships are between input variables and the compressive strength - the scores can be increased if the right relationships are precisely found. Complex relationships with compressive strength that were unable to be identified are possible, in particular for the variables **water**, **fine aggregate** and **coarse aggregate**. Below follows the interpretation from the models.\n",
    "\n",
    "<u>Ensemble methods are too complex to provide help in finding coefficients.</u> Given how RandomForest (RF) works, it is **not possible to convert feature importance(s) of each variable to a coefficient** - hence it is regarded as a 'black box'. Furthermore, a large difference in scores between training and test data would indicate over-fitting on the training data. However, because its predictivity the closest to 100%, the <i>feature importance(s)</i> function will be used to determine which variable is the most important contributor to the overall compressive strength.\n",
    "\n",
    "All predicted models became less accurate for **very high values of compressive strength**. This is expected, due to transformations resulting in under-fitted data, as well as sensitivity to outliers, especially affecting the variables that are highly skewed.\n",
    "  \n",
    "#### Most reasonable choice of regression: **Linear Regression**\n",
    "It outperformed the other simpler regression models with a lower RMSE and higher R squared score.\n",
    "Reference link: https://stephenallwright.com/good-accuracy-score/#:~:text=There%20is%20a%20general%20rule%20when%20it%20comes,and%2070%25%20-%20OK%20Below%2060%25%20-%20Poor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f219fe7fb07c4bb2",
   "metadata": {},
   "source": [
    "### Finding the most important feature to compressive strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93b3b24f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:26:15.920337100Z",
     "start_time": "2024-01-10T23:26:12.660923500Z"
    }
   },
   "outputs": [],
   "source": [
    "'''Using RFRegressorCoeffs and SVRCoeffs as mentioned from above observations.'''\n",
    "LinearCoeffs = RegressionModel(x_train, x_test, y_train, y_test, LinearRegression).Coefficients()\n",
    "RFRegressorCoeffs = RegressionModel(x_train, x_test, y_train, y_test, RandomForestRegressor, n_estimators = 200, max_depth = 15, random_state = 42).Coefficients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fdd0508",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:26:16.569234500Z",
     "start_time": "2024-01-10T23:26:15.932306500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                 0.335159\n",
      "cement              0.334119\n",
      "water               0.130934\n",
      "slag                0.082000\n",
      "superplasticizer    0.052388\n",
      "fineaggregate       0.036575\n",
      "coarseaggregate     0.028825\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(83.94444444444443, 0.5, 'Feature Importance Scores')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Provides the feature importance scores in descending order - a bar graph to help visualise.'''\n",
    "print(pd.Series(RFRegressorCoeffs, index=TCD.columns[:-1]).sort_values(ascending=False))\n",
    "pd.Series(RFRegressorCoeffs, index=TCD.columns[:-1]).sort_values(ascending=False).plot(kind='bar', figsize=(6, 4)) # For visualisation.\n",
    "plt.ylabel('Feature Importance Scores')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e136704",
   "metadata": {},
   "source": [
    "#### **Age** is the most important feature that contributes to the compressive strength."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d709cd666c811f91",
   "metadata": {},
   "source": [
    "### Obtaining the coefficients of each variable for the interactive graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4c41b3e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:26:16.623571800Z",
     "start_time": "2024-01-10T23:26:16.567240100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cement              36.153575\n",
      "slag                11.938024\n",
      "water              -30.917298\n",
      "superplasticizer     8.840093\n",
      "coarseaggregate     -2.674257\n",
      "fineaggregate       -7.222725\n",
      "age                 49.765452\n",
      "dtype: float64\n",
      "0.08254240956260116\n",
      "0.0332165391299257\n",
      "-0.24694327554462367\n",
      "0.27453704505305015\n",
      "-0.007774002112683817\n",
      "-0.018120234103394566\n",
      "0.13671827492402158\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['flyash'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(b)\n\u001b[1;32m      8\u001b[0m \u001b[39m#Taking flyash out as we previously decided\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m DIG \u001b[39m=\u001b[39m DIG\u001b[39m.\u001b[39;49mdrop(\u001b[39m'\u001b[39;49m\u001b[39mflyash\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdrop\u001b[39m(\n\u001b[1;32m   5197\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   5198\u001b[0m     labels: IndexLabel \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   5206\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   5207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   5208\u001b[0m \u001b[39m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5342\u001b[0m \u001b[39m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5343\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5344\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mdrop(\n\u001b[1;32m   5345\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m   5346\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m   5347\u001b[0m         index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   5348\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   5349\u001b[0m         level\u001b[39m=\u001b[39;49mlevel,\n\u001b[1;32m   5350\u001b[0m         inplace\u001b[39m=\u001b[39;49minplace,\n\u001b[1;32m   5351\u001b[0m         errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   5352\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m \u001b[39mfor\u001b[39;00m axis, labels \u001b[39min\u001b[39;00m axes\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4710\u001b[0m     \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 4711\u001b[0m         obj \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39;49m_drop_axis(labels, axis, level\u001b[39m=\u001b[39;49mlevel, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4713\u001b[0m \u001b[39mif\u001b[39;00m inplace:\n\u001b[1;32m   4714\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mdrop(labels, level\u001b[39m=\u001b[39mlevel, errors\u001b[39m=\u001b[39merrors)\n\u001b[1;32m   4752\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m         new_axis \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39;49mdrop(labels, errors\u001b[39m=\u001b[39;49merrors)\n\u001b[1;32m   4754\u001b[0m     indexer \u001b[39m=\u001b[39m axis\u001b[39m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4756\u001b[0m \u001b[39m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6998\u001b[0m \u001b[39mif\u001b[39;00m mask\u001b[39m.\u001b[39many():\n\u001b[1;32m   6999\u001b[0m     \u001b[39mif\u001b[39;00m errors \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m-> 7000\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mlabels[mask]\u001b[39m.\u001b[39mtolist()\u001b[39m}\u001b[39;00m\u001b[39m not found in axis\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   7001\u001b[0m     indexer \u001b[39m=\u001b[39m indexer[\u001b[39m~\u001b[39mmask]\n\u001b[1;32m   7002\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['flyash'] not found in axis\""
     ]
    }
   ],
   "source": [
    "print(pd.Series(LinearCoeffs, index=TCD.columns[:-1]))\n",
    "\n",
    "#'Unscaling' coefficents so that real values of variables can be inputted\n",
    "for a,b in enumerate(LinearCoeffs):\n",
    "    b = b/(DIG.max().iloc[a]-DIG.min().iloc[a])\n",
    "    print(b)\n",
    "\n",
    "#Taking flyash out as we previously decided\n",
    "DIG = DIG.drop('flyash', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08bf2880",
   "metadata": {},
   "source": [
    "# **Interactive Graph**\n",
    "\n",
    "Finally, an interactive graph was implemented in order to best visualise the predictions made from the regression model. The graph allows the user to vary each input and gives a direct prediction to the compressive strength of the concrete over the period of a year. \n",
    "\n",
    "Age was used as the variable in the plot because it was previously noted that this feature was the most important for the predictions. Also from reading around on the subject this was regularly used. We found in our data that the best correlation between the compressive strength and age was found when age was plotted on a logarithmic scale, so for our interactive graph we used $$compressive\\:strength = log{(age)}$$ as a starting point. This was also backed up by various texts such as: https://t.ly/Fzgtn where it is stated that $$f_t = Aln(t)+B$$ where $f_t$ is the compressive strength, $t$ is age, $A = 1.4035ln(B) + 2.9956$ and $B = 0.005(f_c)^{2.20}$. From our regression model we found that $$f_c = m_{cement}\\sqrt{cement} + m_{slag}log(slag) + m_{plasticizer}\\sqrt{plasticizer} + m_{water}\\times water + m_{coarse}\\times coarse + m_{fine}\\times fine$$\n",
    "\n",
    "To best represent the proportionality of each variable to the compressive strength, each was standardised to have a contribution within $0 > x > 1$. Where 0 was the minimum value and 1 was the maximum. This was then multiplied by the coefficiants previously found in the regression model and any functionalities for the best fit applied to the variable in preprocessing was also applied within the $f_t$ function.\n",
    "\n",
    "This gives us a final equation of $$y = f_t = (1.4035ln(0.005(f_c)^{2.20}) + 2.9956)ln(t)+0.005(f_c)^{2.20}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9405bde4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:26:16.877608600Z",
     "start_time": "2024-01-10T23:26:16.582599700Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "def InteractiveGraph(variable_coeff):\n",
    "    '''Plots an interactive graph using the coefficients of our chosen regression model'''\n",
    "    def f(cement, slag, water, superplasticizer, coarseaggregate, fineaggregate, age):\n",
    "        # Using transformations in Preprocessing section and normalisation.\n",
    "        f_t = ((variable_coeff[0]*np.sqrt(cement)/np.sqrt(DIG.max().iloc[0]))\n",
    "              + (variable_coeff[1]*np.log(slag)/np.log(DIG.max().iloc[1]+1))\n",
    "                + (variable_coeff[2]*water/DIG.max().iloc[2])\n",
    "                  + (variable_coeff[3]*(np.sqrt(superplasticizer)/(np.sqrt(DIG.max().iloc[3]))**(4/3)))\n",
    "                    + (variable_coeff[4]*coarseaggregate/DIG.max().iloc[4])\n",
    "                     + (variable_coeff[5]*fineaggregate/DIG.max().iloc[5]))\n",
    "        \n",
    "        y =3.2*(0.005*((f_t)**2.20)*(1.4*np.log(age/28)) + 1.4035*1.4*np.log(0.005*(f_t**2.20))+2.9956)              # Note that flyash is removed. \n",
    "        return y\n",
    "        \n",
    "    # Create subplot\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.subplots_adjust(bottom=0.5) \n",
    "    ax.set(xlabel='Age', ylabel='Compression Strength (MPa)')\n",
    "    plt.ylim([0, 140])\n",
    "\n",
    "    # Create and plot basic strength against age plot using the mean values of each variable for the initial plot\n",
    "    t = np.arange(0.1, 365, 1)\n",
    "    line = f(DIG.mean().iloc[0], DIG.mean().iloc[1], DIG.mean().iloc[2], DIG.mean().iloc[3], DIG.mean().iloc[4], DIG.mean().iloc[5], t)\n",
    "    l, = plt.plot(t, line)\n",
    "\n",
    "    #Create reset button to go back to the mean value\n",
    "    resetax = fig.add_axes([0.025, 0.025, 0.1, 0.04])\n",
    "    button = Button(resetax, 'Reset', color='0.95', hovercolor='0.7')\n",
    "    \n",
    "    #Create sliders for each variable and add them to list Sliders\n",
    "    def create_Sliders():\n",
    "        Sliderlist = []\n",
    "        for i, column in enumerate(TCD.columns[:-2]):\n",
    "            #plot each slider in a given location, each moving up by 0.5 to get a good spread\n",
    "            axvariable = plt.axes([0.25, 0.05*(i + 1), 0.65, 0.03])\n",
    "            var_slider = Slider(axvariable, label = column, valmin = DIG.min().iloc[i], valmax = DIG.max().iloc[i], valinit = DIG.mean().iloc[i])\n",
    "            #Add each slider to Sliders list for access later\n",
    "            Sliderlist.append(var_slider)\n",
    "        return Sliderlist\n",
    "    \n",
    "    #Calling the defined function\n",
    "    Sliders = create_Sliders()\n",
    "\n",
    "    \n",
    "    #The function to be called anytime a slider's value changes which also updates the graph\n",
    "    def update(val):\n",
    "        l.set_ydata(f(Sliders[0].val, Sliders[1].val, Sliders[2].val, Sliders[3].val, Sliders[4].val, Sliders[5].val, t))\n",
    "        ax.autoscale_view(True,True,True)\n",
    "        fig.canvas.draw_idle()\n",
    "    \n",
    "    #Register the update function with each slider\n",
    "    for i in range(len(DIG.columns[:-2])):\n",
    "        Sliders[i].on_changed(update)\n",
    "            \n",
    "    #This function implements the reset button such that the sliders are brought back to there initial value\n",
    "    def reset(event):\n",
    "        for i in range(len(DIG.columns[:-2])):\n",
    "            Sliders[i].reset()\n",
    "    button.on_clicked(reset)\n",
    "\n",
    "    resetax._button = button\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6e0bc8fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-10T23:26:17.308756900Z",
     "start_time": "2024-01-10T23:26:16.887578200Z"
    }
   },
   "outputs": [],
   "source": [
    "'''To run the Interactive Graph - will open a new tab.'''\n",
    "w = InteractiveGraph(LinearCoeffs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "473e7d17",
   "metadata": {},
   "source": [
    "*Note*: Unfortunately we were note able to refine this final graph to predict the strength with great certainty across a range of values. We belive this may be a combination of uncertainty arising from the model found in the paper and our regression model. However, the graph still allows the user to easily visualise of the effects of changing various variables on the compressive strength."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
